{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\51CAR10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0 count hate_speech offensive_language  neither  class   \n",
      "0          0     3           0                  0      3.0    2.0  \\\n",
      "1          1     3           0                  3      0.0    1.0   \n",
      "2          2     3           0                  3      0.0    1.0   \n",
      "3          3     3           0                  2      1.0    1.0   \n",
      "4          4     6           0                  6      0.0    1.0   \n",
      "\n",
      "                                               tweet Unnamed: 7 Unnamed: 8   \n",
      "0  !!! RT @mayasolovely As a woman you shouldn't ...        NaN        NaN  \\\n",
      "1  !!!!! RT @mleew17 boy dats cold...tyga dwn bad...        NaN        NaN   \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...        NaN        NaN   \n",
      "3  !!!!!!!!! RT @C_G_Anderson @viva_based she loo...        NaN        NaN   \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts The shit you ...        NaN        NaN   \n",
      "\n",
      "  Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14   \n",
      "0        NaN         NaN         NaN         NaN         NaN         NaN  \\\n",
      "1        NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "2        NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "3        NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "4        NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "  Unnamed: 15 Unnamed: 16 Unnamed: 17  \n",
      "0         NaN         NaN         NaN  \n",
      "1         NaN         NaN         NaN  \n",
      "2         NaN         NaN         NaN  \n",
      "3         NaN         NaN         NaN  \n",
      "4         NaN         NaN         NaN  \n"
     ]
    }
   ],
   "source": [
    "stopword=set(stopwords.words('english'))\n",
    "data = pd.read_csv(\"./twitter.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0 count hate_speech offensive_language  neither  class   \n",
      "0          0     3           0                  0      3.0    2.0  \\\n",
      "1          1     3           0                  3      0.0    1.0   \n",
      "2          2     3           0                  3      0.0    1.0   \n",
      "3          3     3           0                  2      1.0    1.0   \n",
      "4          4     6           0                  6      0.0    1.0   \n",
      "\n",
      "                                               tweet Unnamed: 7 Unnamed: 8   \n",
      "0  !!! RT @mayasolovely As a woman you shouldn't ...        NaN        NaN  \\\n",
      "1  !!!!! RT @mleew17 boy dats cold...tyga dwn bad...        NaN        NaN   \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...        NaN        NaN   \n",
      "3  !!!!!!!!! RT @C_G_Anderson @viva_based she loo...        NaN        NaN   \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts The shit you ...        NaN        NaN   \n",
      "\n",
      "  Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14   \n",
      "0        NaN         NaN         NaN         NaN         NaN         NaN  \\\n",
      "1        NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "2        NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "3        NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "4        NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "  Unnamed: 15 Unnamed: 16 Unnamed: 17                 labels  \n",
      "0         NaN         NaN         NaN  No Hate and Offensive  \n",
      "1         NaN         NaN         NaN     Offensive Language  \n",
      "2         NaN         NaN         NaN     Offensive Language  \n",
      "3         NaN         NaN         NaN     Offensive Language  \n",
      "4         NaN         NaN         NaN     Offensive Language  \n"
     ]
    }
   ],
   "source": [
    "data[\"labels\"] = data[\"class\"].map({0: \"Hate Speech\", \n",
    "                                    1: \"Offensive Language\", \n",
    "                                    2: \"No Hate and Offensive\"})\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet                 labels\n",
      "0  !!! RT @mayasolovely As a woman you shouldn't ...  No Hate and Offensive\n",
      "1  !!!!! RT @mleew17 boy dats cold...tyga dwn bad...     Offensive Language\n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     Offensive Language\n",
      "3  !!!!!!!!! RT @C_G_Anderson @viva_based she loo...     Offensive Language\n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts The shit you ...     Offensive Language\n"
     ]
    }
   ],
   "source": [
    "data = data[[\"tweet\", \"labels\"]]\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'dropna'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data\u001b[38;5;241m=\u001b[39m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m()\u001b[38;5;241m.\u001b[39mset_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'dropna'"
     ]
    }
   ],
   "source": [
    "#data=data.dropna().set_axis=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text=\" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text=\" \".join(text)\n",
    "    return text\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         rt mayasolov woman shouldnt complain clean ho...\n",
       "1         rt  boy dat coldtyga dwn bad cuffin dat hoe  ...\n",
       "2         rt urkindofbrand dawg rt  ever fuck bitch sta...\n",
       "3                   rt cganderson vivabas look like tranni\n",
       "4         rt shenikarobert shit hear might true might f...\n",
       "                               ...                        \n",
       "26948     yous muthafin lie   coreyemanuel right tl trash \n",
       "26949                     youv gone broke wrong heart babi\n",
       "26950    young buck wanna eat dat nigguh like aint fuck...\n",
       "26951                       youu got wild bitch tellin lie\n",
       "26952    ruffl  ntac eileen dahlia  beauti color combin...\n",
       "Name: tweet, Length: 24781, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(data[\"tweet\"])\n",
    "y = np.array(data[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(x) # Fit the Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m clf \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\51CAR10\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    890\u001b[0m         X,\n\u001b[0;32m    891\u001b[0m         y,\n\u001b[0;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\51CAR10\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:186\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    184\u001b[0m check_X_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(dtype\u001b[39m=\u001b[39mDTYPE, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    185\u001b[0m check_y_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 186\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    187\u001b[0m     X, y, validate_separately\u001b[39m=\u001b[39;49m(check_X_params, check_y_params)\n\u001b[0;32m    188\u001b[0m )\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[0;32m    190\u001b[0m     X\u001b[39m.\u001b[39msort_indices()\n",
      "File \u001b[1;32mc:\\Users\\51CAR10\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:582\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_y_params:\n\u001b[0;32m    581\u001b[0m         check_y_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params}\n\u001b[1;32m--> 582\u001b[0m     y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_y_params)\n\u001b[0;32m    583\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    584\u001b[0m     X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\51CAR10\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[0;32m    922\u001b[0m             array,\n\u001b[0;32m    923\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    924\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    925\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    928\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\51CAR10\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:111\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n\u001b[0;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m _object_dtype_isnan(X)\u001b[39m.\u001b[39many():\n\u001b[1;32m--> 111\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInput contains NaN\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[39m# We need only consider float arrays, hence can early return for all else.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mfc\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(sample):\n",
    "    data = cv.transform([sample]).toarray()\n",
    "    pre_dict=clf.predict(data)\n",
    "    return pre_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://d0dec224ca3e6dfb5a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d0dec224ca3e6dfb5a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_face=gd.Interface(fn=predictor,inputs=\"text\",outputs=\"text\",title=\"HATE Speech APP\")\n",
    "inter_face.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfad3e0b994dc944cc40b27ba79c4bf5ee3358aade9be69ac469d504c3fa6822"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
